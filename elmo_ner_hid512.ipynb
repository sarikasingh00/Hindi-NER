{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "stable-navigator",
   "metadata": {},
   "source": [
    "https://www.depends-on-the-definition.com/named-entity-recognition-with-residual-lstm-and-elmo/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "apparent-banner",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cleared-sword",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = r'Data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bottom-farmer",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['अरे', 'O'],\n",
       "       ['सारथी', 'O'],\n",
       "       ['जम्मू', 'U-location'],\n",
       "       ...,\n",
       "       ['क्या', 'O'],\n",
       "       ['है', 'O'],\n",
       "       ['?', 'O']], dtype='<U14')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = np.loadtxt(fname=data_dir+'nertrnweather.txt',encoding='utf-8',dtype=str)\n",
    "file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "unsigned-operation",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['Sentence #','Word','Tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "helpful-tomato",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence 1</td>\n",
       "      <td>अरे</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence 1</td>\n",
       "      <td>सारथी</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentence 1</td>\n",
       "      <td>जम्मू</td>\n",
       "      <td>U-location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sentence 1</td>\n",
       "      <td>कैसा</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sentence 1</td>\n",
       "      <td>मौसम</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1403</th>\n",
       "      <td>Sentence 203</td>\n",
       "      <td>की</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1404</th>\n",
       "      <td>Sentence 203</td>\n",
       "      <td>स्थिति</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1405</th>\n",
       "      <td>Sentence 203</td>\n",
       "      <td>क्या</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1406</th>\n",
       "      <td>Sentence 203</td>\n",
       "      <td>है</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1407</th>\n",
       "      <td>Sentence 203</td>\n",
       "      <td>?</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1408 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Sentence #    Word         Tag\n",
       "0       Sentence 1     अरे           O\n",
       "1       Sentence 1   सारथी           O\n",
       "2       Sentence 1   जम्मू  U-location\n",
       "3       Sentence 1    कैसा           O\n",
       "4       Sentence 1    मौसम           O\n",
       "...            ...     ...         ...\n",
       "1403  Sentence 203      की           O\n",
       "1404  Sentence 203  स्थिति           O\n",
       "1405  Sentence 203    क्या           O\n",
       "1406  Sentence 203      है           O\n",
       "1407  Sentence 203       ?           O\n",
       "\n",
       "[1408 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=1\n",
    "data = list()\n",
    "with open(data_dir+'nertrnweather.txt',encoding='utf-8') as f:\n",
    "    for line in f.readlines():\n",
    "        if line=='\\n':\n",
    "            i+=1\n",
    "        else:\n",
    "            data=line.split(\" \")\n",
    "            df=df.append({\"Sentence #\":f\"Sentence {i}\",\"Word\":data[0],\"Tag\":re.sub(\"\\n\",\"\",data[1])},ignore_index=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "norwegian-familiar",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "282"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = list(set(df[\"Word\"].values))\n",
    "words.append(\"ENDPAD\")\n",
    "n_words = len(words); n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "double-reply",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-date',\n",
       " 'O',\n",
       " 'B-weather_type',\n",
       " 'L-date',\n",
       " 'L-weather_type',\n",
       " 'B-location',\n",
       " 'U-date',\n",
       " 'L-location',\n",
       " 'U-location',\n",
       " 'U-weather_type']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = list(set(df[\"Tag\"].values))\n",
    "n_tags = len(tags); n_tags\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "champion-great",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceGetter(object):\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.n_sent = 1\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, t) for w, t in zip(s[\"Word\"].values.tolist(),\n",
    "                                                           s[\"Tag\"].values.tolist())]\n",
    "        self.grouped = self.data.groupby(\"Sentence #\").apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "    \n",
    "    def get_next(self):\n",
    "        try:\n",
    "            s = self.grouped[\"Sentence {}\".format(self.n_sent)]\n",
    "            self.n_sent += 1\n",
    "            return s\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "german-pierre",
   "metadata": {},
   "outputs": [],
   "source": [
    "getter = SentenceGetter(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "express-reaction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('अरे', 'O'), ('सारथी', 'O'), ('जम्मू', 'U-location'), ('कैसा', 'O'), ('मौसम', 'O'), ('है', 'O')]\n"
     ]
    }
   ],
   "source": [
    "sent = getter.get_next()\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "possible-lincoln",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = getter.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "headed-nutrition",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 16\n",
    "tag2idx = {t: i for i, t in enumerate(tags)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "american-armstrong",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag2idx[\"U-location\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "funded-amsterdam",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [[w[0] for w in s] for s in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aerial-convergence",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X = []\n",
    "for seq in X:\n",
    "    new_seq = []\n",
    "    for i in range(max_len):\n",
    "        try:\n",
    "            new_seq.append(seq[i])\n",
    "        except:\n",
    "            new_seq.append(\"__PAD__\")\n",
    "    new_X.append(new_seq)\n",
    "X = new_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "floppy-sharing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203\n"
     ]
    }
   ],
   "source": [
    "print(len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "discrete-donna",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [[tag2idx[w[1]] for w in s] for s in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "adopted-morris",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "y = pad_sequences(maxlen=max_len, sequences=y, padding=\"post\", value=tag2idx[\"O\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "going-identification",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "individual-number",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "perceived-helicopter",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.14, random_state=2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "detected-gossip",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "accessible-hartford",
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.modules.elmo import Elmo,batch_to_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "plastic-angel",
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = os.path.join('hi', 'elmo')\n",
    "options_file = os.path.join(datadir, 'hi-d512-options.json')\n",
    "weight_file = os.path.join(datadir, 'hi-d512-elmo.hdf5')\n",
    "\n",
    "elmo = Elmo(options_file,weight_file,1,dropout=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "thermal-lunch",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds = elmo(batch_to_ids(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "potential-connection",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = embeds[\"elmo_representations\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "identical-cemetery",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([203, 16, 1024])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "numeric-celtic",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tensorflow as tf\n",
    "\n",
    "embedding1 = []\n",
    "for t in embedding:\n",
    "    np_tensor = t.detach().numpy()\n",
    "    embedding1 += [np_tensor.tolist()]\n",
    "\n",
    "embedding = embedding1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "hybrid-travel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203\n"
     ]
    }
   ],
   "source": [
    "print(len(embedding[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "antique-science",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(203, 16)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "tribal-blues",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, X_te, y_tr, y_te = train_test_split(embedding[0], y, test_size=0.14, random_state=2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "departmental-rapid",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf\n",
    "import tensorflow_hub as hub\n",
    "from keras import backend as K\n",
    "# import keras.backend.tensorflow_backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "sublime-remove",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "# K.set_session(sess)\n",
    "tf.compat.v1.keras.backend.set_session(sess);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "spare-gateway",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.disable_eager_execution()\n",
    "# tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "extra-hollywood",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, Input\n",
    "from keras.layers.merge import add\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional, Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "advance-kernel",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = Input(shape=(max_len,1024), dtype=tf.float32)\n",
    "# embedding = Lambda(ElmoEmbedding, output_shape=(None, 1024))(input_text)\n",
    "# embedding = embedding_layer(input_text)\n",
    "# embedding = ElmoEmbeddingLayer()(input_text)\n",
    "# print(embedding)\n",
    "# embedding = Lambda(ElmoEmbedding,output_shape=(None, 1024))(input_text)\n",
    "# embedding = Lambda(ElmoEmbedding,output_shape=(None, 1024))\n",
    "# x = Bidirectional(LSTM(units=512, return_sequences=True,\n",
    "#                        recurrent_dropout=0.2, dropout=0.2))(embedding)\n",
    "x = Bidirectional(LSTM(units=512, return_sequences=True,\n",
    "                       recurrent_dropout=0.2, dropout=0.2))(input_text)\n",
    "x_rnn = Bidirectional(LSTM(units=512, return_sequences=True,\n",
    "                           recurrent_dropout=0.2, dropout=0.2))(x)\n",
    "x = add([x, x_rnn])  # residual connection to the first biLSTM\n",
    "out = TimeDistributed(Dense(n_tags, activation=\"softmax\"))(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "former-suspension",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_text = Input(shape=(max_len,), dtype=tf.string)\n",
    "model = Model(input_text, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "material-acting",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bronze-fever",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tr = y_tr.reshape(y_tr.shape[0], y_tr.shape[1], 1)\n",
    "y_te = y_te.reshape(y_te.shape[0], y_te.shape[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "approved-conditioning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29, 16, 1024)\n",
      "(174, 16, 1024)\n"
     ]
    }
   ],
   "source": [
    "print(np.array(X_te).shape)\n",
    "print(np.array(X_tr).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "prerequisite-kitty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 174 samples, validate on 29 samples\n",
      "Epoch 1/5\n",
      "174/174 [==============================] - ETA: 0s - loss: 0.9813 - accuracy: 0.7015"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Sarika\\NER\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "174/174 [==============================] - 22s 128ms/sample - loss: 0.9813 - accuracy: 0.7015 - val_loss: 0.3515 - val_accuracy: 0.8750\n",
      "Epoch 2/5\n",
      "174/174 [==============================] - 16s 93ms/sample - loss: 0.2782 - accuracy: 0.9131 - val_loss: 0.1770 - val_accuracy: 0.9634\n",
      "Epoch 3/5\n",
      "174/174 [==============================] - 16s 92ms/sample - loss: 0.1441 - accuracy: 0.9677 - val_loss: 0.1019 - val_accuracy: 0.9655\n",
      "Epoch 4/5\n",
      "174/174 [==============================] - 16s 94ms/sample - loss: 0.0857 - accuracy: 0.9756 - val_loss: 0.0777 - val_accuracy: 0.9806\n",
      "Epoch 5/5\n",
      "174/174 [==============================] - 16s 94ms/sample - loss: 0.0619 - accuracy: 0.9878 - val_loss: 0.0633 - val_accuracy: 0.9849\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(np.array(X_tr), y_tr, validation_data=(np.array(X_te), y_te),\n",
    "                    batch_size=batch_size, epochs=5, verbose=1)\n",
    "# history = model.fit(np.array(embedding), y_tr, validation_data=(np.array(X_te), y_te),\n",
    "#                     batch_size=batch_size, epochs=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "framed-trainer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(\"ner_elmo_bilstm.h5\")\n",
    "# model.save('ner_elmo_bilstm.h5', overwrite=True,  save_format='tf', include_optimizer=True,)\n",
    "# model.save_weights('ner_elmo_bilstm_weights.h5')\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bridal-sample",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, 16, 1024)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(X_te[i:i+batch_size]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "local-transaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "p = model.predict(np.array(X_te[i:i+batch_size]))\n",
    "p = np.argmax(p, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "civil-indonesia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29, 16)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "joined-information",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence  120\n",
      " सिडनी (U-location) 8 का (O) 1 मौसम (O) 1\n",
      "Sentence  72\n",
      " किधर (O) 1 ज्यादा (O) 1 ठंडी (U-weather_type) 9 होगी (O) 1 कसोल (U-location) 8 या (O) 1 मनाली (U-location) 8\n",
      "Sentence  152\n",
      " आज (U-date) 6 गर्मी (U-weather_type) 9 ज्यादा (O) 1 है (O) 1 क्या (O) 1\n",
      "Sentence  84\n",
      " कहा (O) 1 मौसम (O) 1 अच्छा (O) 1 है (O) 1 बेंगलोरे (U-location) 8 या (O) 1 गुड़गांव (U-location) 8\n",
      "Sentence  66\n",
      " जौनपुर (U-location) 8 कितना (O) 1 ज्यादा (O) 1 गर्म (U-weather_type) 9 है (O) 1 अभी (U-date) 6 सूरत (U-location) 8 से (O) 1\n",
      "Sentence  178\n",
      " क्या (O) 1 मैं (O) 1 घर (O) 1 के (O) 1 रास्ते (O) 1 पर (O) 1 भीगने (O) 1 वाला (O) 1 हूं (O) 1\n",
      "Sentence  5\n",
      " आज (U-date) 6 बरसात (U-weather_type) 9 की (O) 1 संभावना (O) 1 है (O) 1 क्या (O) 1\n",
      "Sentence  165\n",
      " और (O) 1 कितना (O) 1 गर्मी (U-weather_type) 9 बढ़ेगा (O) 1 और (O) 1 यहाँ (O) 1\n",
      "Sentence  2\n",
      " और (O) 1 कितनी (O) 1 बारिश (U-weather_type) 9 होगी (O) 1 यहाँ (O) 1\n",
      "Sentence  37\n",
      " सर्दी (U-weather_type) 9 कब (O) 1 से (O) 1 चालू (O) 1 होगी (O) 1 इस (B-date) 0 बार (L-date) 3\n",
      "Sentence  130\n",
      " आज (U-date) 6 मौसम (O) 1 का (O) 1 क्या (O) 1 हाल (O) 1 है (O) 1\n",
      "Sentence  126\n",
      " आज (U-date) 6 के (O) 1 लिए (O) 1 मौसम (O) 1 रिपोर्ट (O) 1 बताओ (O) 1\n",
      "Sentence  174\n",
      " क्या (O) 1 इस (B-date) 0 बार (L-date) 3 ज्यादा (O) 1 गर्मी (U-weather_type) 9 पड़ेगी (O) 1 सूरत (U-location) 8 में (O) 1\n",
      "Sentence  28\n",
      " कितनी (O) 1 बर्फ (U-weather_type) 9 गिर (O) 1 रही (O) 1 है (O) 1 कसोल (U-location) 8 में (O) 1\n",
      "Sentence  11\n",
      " ये (O) 1 बारिश (U-weather_type) 9 कब (O) 1 बंद (O) 1 होगी (O) 1\n",
      "Sentence  54\n",
      " जम्मू (U-location) 8 में (O) 1 मौसम (O) 1 अपेक्षाकृत (O) 1 कैसा (O) 1 है (O) 1\n",
      "Sentence  169\n",
      " अभी (U-date) 6 कितनी (O) 1 गर्मी (U-weather_type) 9 है (O) 1 भरुच (U-location) 8 में (O) 1\n",
      "Sentence  92\n",
      " क्या (O) 1 कल (U-date) 6 ज्यादा (O) 1 गर्म (U-weather_type) 9 होगा (O) 1 दिल्ली (U-location) 8 में (O) 1 आज (U-date) 6 से (O) 1\n",
      "Sentence  143\n",
      " कसोल (U-location) 8 में (O) 1 क्या (O) 1 तापमान (O) 1 चल (O) 1 रहा (O) 1 है (O) 1\n",
      "Sentence  62\n",
      " क्या (O) 1 मुंबई (U-location) 8 ज्यादा (O) 1 गर्म (U-weather_type) 9 है (O) 1 कोलकाता (U-location) 8 से (O) 1\n",
      "Sentence  177\n",
      " क्या (O) 1 बाद (O) 1 में (O) 1 बारिश (U-weather_type) 9 होगी (O) 1\n",
      "Sentence  182\n",
      " क्या (O) 1 बाद (O) 1 में (O) 1 बारिश (U-weather_type) 9 होगी (O) 1\n",
      "Sentence  85\n",
      " हमीरपुर (U-location) 8 गरम (U-weather_type) 9 है (O) 1 क्या (O) 1 नागपुर (U-location) 8 से (O) 1\n",
      "Sentence  190\n",
      " क्या (O) 1 रात (L-date) 3 भर (O) 1 बारिश (U-weather_type) 9 होगी (O) 1\n",
      "Sentence  93\n",
      " क्या (O) 1 कल (U-date) 6 भी (O) 1 काफी (O) 1 बारिश (U-weather_type) 9 होगी (O) 1 आज (U-date) 6 के (O) 1 जैसे (O) 1\n",
      "Sentence  194\n",
      " क्या (O) 1 बिना (O) 1 छतरी (O) 1 के (O) 1 रात (O) 1 में (O) 1 यात्रा (O) 1 करना (O) 1 सुरक्षित (O) 1 है (O) 1\n",
      "Sentence  202\n",
      " कब (O) 1 तक (O) 1 बारिश (U-weather_type) 9 होगी (O) 1 दिल्ली (U-location) 8 में (O) 1\n",
      "Sentence  51\n",
      " अप्रैल (U-date) 6 में (O) 1 बर्फ (U-weather_type) 9 गिर (O) 1 सकती (O) 1 है (O) 1 क्या (O) 1 शिमला (U-location) 8 में (O) 1\n",
      "Sentence  53\n",
      " कौन (O) 1 अधिक (O) 1 ठंडा (U-weather_type) 9 है (O) 1 , (O) 1 कसोल (U-location) 8 या (O) 1 गंगटोक (U-location) 8\n",
      "Sentence  58\n",
      " मुंबई (U-location) 8 या (O) 1 चेन्नई (U-location) 8 में (O) 1 आज (U-date) 6 ज्यादा (O) 1 गर्म (U-weather_type) 9\n"
     ]
    }
   ],
   "source": [
    "for X_te1, y_te1,p1 in zip(X_te,y_te.tolist(),p):\n",
    "    for i,e in enumerate(embedding[0]):\n",
    "        if e==X_te1:\n",
    "            print(\"Sentence \", i)\n",
    "            sentence = \"\"\n",
    "#             c+=1\n",
    "            for w, true, pred in zip(X[i], y_te1, p1):\n",
    "                if w != \"__PAD__\":\n",
    "                    sentence += \" {} ({}) {}\".format(w,tags[pred],pred)\n",
    "#                 if w != \"__PAD__\" and tags[pred] == tags[true[0]]:\n",
    "#                     print(\"{} : {} ({})\".format(w,tags[pred], tags[true[0]]))\n",
    "            print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "national-conditions",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "p = model.predict(np.array(X_tr[i:i+batch_size]))\n",
    "p = np.argmax(p, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lonely-anxiety",
   "metadata": {},
   "outputs": [],
   "source": [
    "for X_te1, y_te1,p1 in zip(X_tr,y_tr.tolist(),p):\n",
    "    for i,e in enumerate(embedding[0]):\n",
    "        if e==X_te1:\n",
    "            print(\"Sentence \", i)\n",
    "            sentence = \"\"\n",
    "#             c+=1\n",
    "            for w, true, pred in zip(X[i], y_te1, p1):\n",
    "                if w != \"__PAD__\":\n",
    "                    sentence += \" {} ({})\".format(w,tags[pred])\n",
    "#                 if w != \"__PAD__\" and tags[pred] == tags[true[0]]:\n",
    "#                     print(\"{} : {} ({})\".format(w,tags[pred], tags[true[0]]))\n",
    "            print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlikely-fellow",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['Sentence #','Word','Tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outstanding-composition",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=1\n",
    "data = list()\n",
    "with open(data_dir+'nervalweather.txt',encoding='utf-8') as f:\n",
    "    for line in f.readlines():\n",
    "        if line=='\\n':\n",
    "            i+=1\n",
    "        else:\n",
    "            data=line.split(\" \")\n",
    "            df=df.append({\"Sentence #\":f\"Sentence {i}\",\"Word\":data[0],\"Tag\":re.sub(\"\\n\",\"\",data[1])},ignore_index=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attempted-morris",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(set(df[\"Word\"].values))\n",
    "words.append(\"ENDPAD\")\n",
    "n_words = len(words); n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quarterly-detroit",
   "metadata": {},
   "outputs": [],
   "source": [
    "getter = SentenceGetter(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instrumental-hughes",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = getter.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatal-thursday",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 16\n",
    "tag2idx = {t: i for i, t in enumerate(tags)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "completed-score",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = [[w[0] for w in s] for s in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foreign-allen",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X = []\n",
    "for seq in X_test:\n",
    "    new_seq = []\n",
    "    for i in range(max_len):\n",
    "        try:\n",
    "            new_seq.append(seq[i])\n",
    "        except:\n",
    "            new_seq.append(\"__PAD__\")\n",
    "    new_X.append(new_seq)\n",
    "X_test = new_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innovative-print",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = [[tag2idx[w[1]] for w in s] for s in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decreased-organizer",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "y_test = pad_sequences(maxlen=max_len, sequences=y_test, padding=\"post\", value=tag2idx[\"O\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "digital-train",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_test))\n",
    "X_test += X_test[0:8]\n",
    "print(len(X_test))\n",
    "\n",
    "#making size = batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legitimate-lobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds = elmo(batch_to_ids(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "about-option",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = embeds[\"elmo_representations\"]\n",
    "# embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hungarian-blink",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tensorflow as tf\n",
    "\n",
    "embedding1 = []\n",
    "for t in embedding:\n",
    "    np_tensor = t.detach().numpy()\n",
    "    embedding1 += [np_tensor.tolist()]\n",
    "\n",
    "embedding = embedding1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lovely-vancouver",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "\n",
    "p = model.predict(np.array(embedding[0]))\n",
    "p = np.argmax(p, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "economic-board",
   "metadata": {},
   "outputs": [],
   "source": [
    "for X_te1, y_te1,p1 in zip(embedding[0],y_test.tolist(),p):\n",
    "    for i,e in enumerate(embedding[0]):\n",
    "        if e==X_te1:\n",
    "            print(\"Sentence \", i)\n",
    "            sentence = \"\"\n",
    "#             c+=1\n",
    "            for w, true, pred in zip(X_test[i], y_te1, p1):\n",
    "                if w != \"__PAD__\":\n",
    "                    sentence += \" {} ({})\".format(w,tags[pred])\n",
    "#                 if w != \"__PAD__\" and tags[pred] == tags[true[0]]:\n",
    "#                     print(\"{} : {} ({})\".format(w,tags[pred], tags[true[0]]))\n",
    "            print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aboriginal-syracuse",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coral-assistant",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
